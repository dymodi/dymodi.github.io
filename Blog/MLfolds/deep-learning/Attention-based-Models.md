---
layout: single
title: "Attention-based Models"
permalink: /blog/MLfolds/deep-learning/Attention-based-Models/
classes: wide
author_profile: true
use_math: true
date: 19/12/25
---


A beginner' guide on attention-based models can be found [here](https://krntneja.github.io/posts/2018/attention-based-models-1).

For learning on sequential inputs, attention-based methods can allow us to use variable input length.

In short, the idea is that in generating context $$c_i$$, decoder RNN pay special attention on some but not all hidden states $$h_j$$.

The attention mechanism is various. For example, different probabilities can be assigned to $$\textbf{h}$$.



## Ref.

